{"cells":[{"cell_type":"code","metadata":{"source_hash":"7e6da797","execution_start":1739515787475,"execution_millis":9981,"execution_context_id":"403d3629-6d29-4bae-8934-bd38e60142b8","cell_id":"a24cb0d29346404c83e8a3201a19168d","deepnote_cell_type":"code"},"source":"!pip install opencv-python==4.11.0.86","block_group":"7d407b98a61a43a4a17e36da32733269","execution_count":3,"outputs":[{"name":"stdout","text":"Collecting opencv-python==4.11.0.86\n  Downloading opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (63.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.0/63.0 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.17.0 in /root/venv/lib/python3.9/site-packages (from opencv-python==4.11.0.86) (1.25.2)\nInstalling collected packages: opencv-python\nSuccessfully installed opencv-python-4.11.0.86\n\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/fe4953d1-11d1-48d6-88cb-f58f713b2477","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"794ee44f","execution_start":1739515868881,"execution_millis":2266,"execution_context_id":"403d3629-6d29-4bae-8934-bd38e60142b8","cell_id":"5aaa4326b9754c07abb3aec19b28cb05","deepnote_cell_type":"code"},"source":"# To fix the error 'ImportError: libGL.so.1: cannot open shared object file...', we need to install 'libgl1-mesa-glx' which provides the missing shared library.\n# This can be done by running the following command in the notebook or terminal.\n\n# !apt-get update\n!apt-get install -y libgl1-mesa-glx","block_group":"5aaa4326b9754c07abb3aec19b28cb05","execution_count":7,"outputs":[{"name":"stdout","text":"\n\n\nE: Unable to locate package libgl1-mesa-glx\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/0fd5beed-4d53-4ec9-b86d-bfd9469aa84b","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"707a7b6d","execution_start":1739516121835,"execution_millis":5759,"execution_context_id":"95fbaa8f-6aad-4ff3-842e-af890854bd5e","cell_id":"f30f75eef82443bfa251913cede9d9bf","deepnote_cell_type":"code"},"source":"import cv2\nimport numpy as np\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# Function to preprocess an image\ndef preprocess_image(image_path, target_size=(128, 128)):\n    img = cv2.imread(image_path)\n    img = cv2.resize(img, target_size) / 255.0  # Normalize\n    return img\n\n# Image augmentation\ndatagen = ImageDataGenerator(rotation_range=20, width_shift_range=0.1, height_shift_range=0.1,\n                             zoom_range=0.2, horizontal_flip=True)","block_group":"f86fdce1296949309c3dafe3dc5740e2","execution_count":1,"outputs":[{"name":"stderr","text":"2025-02-14 06:55:22.652058: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2025-02-14 06:55:22.660439: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n2025-02-14 06:55:22.717628: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-02-14 06:55:22.717781: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-02-14 06:55:22.719221: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-02-14 06:55:22.728109: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n2025-02-14 06:55:22.733361: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2025-02-14 06:55:25.032949: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/de70b82f-b322-4ea0-8e29-a2d6d3c3090b","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"498c39dc","execution_start":1739521433602,"execution_millis":361,"execution_context_id":"d8bd1f29-c448-4ee7-8d85-4aa5637721ca","cell_id":"e2fc5f1733184d43aca0e729a4c6698b","deepnote_cell_type":"code"},"source":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n\n# Define species list (replace with actual species in your dataset)\nspecies_list = [\"coyote\", \"elephant\",\"lion\"]  # Example species\nnum_classes = len(species_list)\n\n# Build CNN Model\nmodel = Sequential([\n    Conv2D(32, (3,3), activation='relu', input_shape=(128, 128, 3)),\n    MaxPooling2D(2,2),\n    Conv2D(64, (3,3), activation='relu'),\n    MaxPooling2D(2,2),\n    Flatten(),\n    Dense(128, activation='relu'),\n    Dense(num_classes, activation='softmax')  # Corrected\n])\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# Model summary\nmodel.summary()","block_group":"53d7309e368a41b1b5135c03e1d3feb3","execution_count":5,"outputs":[{"name":"stdout","text":"Model: \"sequential_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d_2 (Conv2D)           (None, 126, 126, 32)      896       \n                                                                 \n max_pooling2d_2 (MaxPoolin  (None, 63, 63, 32)        0         \n g2D)                                                            \n                                                                 \n conv2d_3 (Conv2D)           (None, 61, 61, 64)        18496     \n                                                                 \n max_pooling2d_3 (MaxPoolin  (None, 30, 30, 64)        0         \n g2D)                                                            \n                                                                 \n flatten_1 (Flatten)         (None, 57600)             0         \n                                                                 \n dense_2 (Dense)             (None, 128)               7372928   \n                                                                 \n dense_3 (Dense)             (None, 3)                 387       \n                                                                 \n=================================================================\nTotal params: 7392707 (28.20 MB)\nTrainable params: 7392707 (28.20 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/062ee03a-7d83-40f4-a56f-ce8cf76fc2d4","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"ffd40d0b","execution_start":1739520134859,"execution_millis":3123,"execution_context_id":"d8bd1f29-c448-4ee7-8d85-4aa5637721ca","cell_id":"0b3e7664e1db40b9b65ee96eede1ca96","deepnote_cell_type":"code"},"source":"","block_group":"5c77507849e740919f1d9bb9a0e709c8","execution_count":1,"outputs":[{"name":"stdout","text":"Dataset folders for Lion, Coyote, and Elephant created successfully!\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/b3e1c202-a73d-49ce-beff-8acedaebaacf","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"8160f2e2","execution_start":1739521544446,"execution_millis":597181,"execution_context_id":"d8bd1f29-c448-4ee7-8d85-4aa5637721ca","cell_id":"b777a16b788b40d280c4dd1e4e86f349","deepnote_cell_type":"code"},"source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\ndatagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n\ntrain_generator = datagen.flow_from_directory(\"footprints_dataset\", target_size=(128, 128),\n                                              batch_size=32, class_mode='sparse', subset='training')\n\nval_generator = datagen.flow_from_directory(\"footprints_dataset\", target_size=(128, 128),\n                                            batch_size=32, class_mode='sparse', subset='validation')\n\nmodel.fit(train_generator, validation_data=val_generator, epochs=10)\nmodel.save(\"wildlife_model.h5\")","block_group":"e58f1ec45fad4c8faad300eb008f0c2f","execution_count":7,"outputs":[{"name":"stdout","text":"Found 116 images belonging to 2 classes.\nFound 28 images belonging to 2 classes.\nEpoch 1/10\n4/4 [==============================] - 45s 12s/step - loss: 3.6955 - accuracy: 0.6466 - val_loss: 0.6574 - val_accuracy: 0.8571\nEpoch 2/10\n4/4 [==============================] - 47s 13s/step - loss: 0.6934 - accuracy: 0.4828 - val_loss: 0.4823 - val_accuracy: 0.8571\nEpoch 3/10\n4/4 [==============================] - 46s 12s/step - loss: 0.4919 - accuracy: 0.8276 - val_loss: 0.4073 - val_accuracy: 0.8571\nEpoch 4/10\n4/4 [==============================] - 47s 14s/step - loss: 0.4437 - accuracy: 0.8276 - val_loss: 0.4472 - val_accuracy: 0.8571\nEpoch 5/10\n4/4 [==============================] - 51s 13s/step - loss: 0.4202 - accuracy: 0.8276 - val_loss: 0.4637 - val_accuracy: 0.8571\nEpoch 6/10\n4/4 [==============================] - 43s 11s/step - loss: 0.3835 - accuracy: 0.8276 - val_loss: 0.4779 - val_accuracy: 0.8571\nEpoch 7/10\n4/4 [==============================] - 47s 14s/step - loss: 0.3315 - accuracy: 0.8276 - val_loss: 0.4467 - val_accuracy: 0.8571\nEpoch 8/10\n4/4 [==============================] - 44s 11s/step - loss: 0.2790 - accuracy: 0.8276 - val_loss: 0.4440 - val_accuracy: 0.8571\nEpoch 9/10\n4/4 [==============================] - 45s 12s/step - loss: 0.2183 - accuracy: 0.9569 - val_loss: 0.5305 - val_accuracy: 0.8571\nEpoch 10/10\n4/4 [==============================] - 48s 12s/step - loss: 0.1983 - accuracy: 0.9138 - val_loss: 0.4678 - val_accuracy: 0.8571\n/root/venv/lib/python3.9/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n  saving_api.save_model(\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/6695bc84-3825-49f0-bfc2-105e3ae7f39a","content_dependencies":null},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=540cdf78-a474-4530-b428-3245ce46c724' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote_persisted_session":{"createdAt":"2025-02-14T07:16:47.901Z"},"deepnote_notebook_id":"1c5652cb1d8b4836a34e6ef6869d63c5"}}